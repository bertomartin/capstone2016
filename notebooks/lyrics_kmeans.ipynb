{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import  MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.lyrics\n",
    "coll = db.yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>title</th>\n",
       "      <th>track_id</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5827dca77aa2eb0ad91b8fdc</td>\n",
       "      <td>Bukka White</td>\n",
       "      <td>I was over in Aberdeen\\nOn my way to New Orlea...</td>\n",
       "      <td>Aberdeen Mississippi Blues</td>\n",
       "      <td>TRHRKYP128F4280BB1</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5827dca77aa2eb0ad91b8fdd</td>\n",
       "      <td>Bukka White</td>\n",
       "      <td>When a man gets trouble in his mind\\nHe wanna ...</td>\n",
       "      <td>Sleepy Man Blues</td>\n",
       "      <td>TRCAHZD128F4280BC1</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5827dca77aa2eb0ad91b8fde</td>\n",
       "      <td>Bessie Smith</td>\n",
       "      <td>Woke up this mornin' when chickens was crowin'...</td>\n",
       "      <td>Young Woman's Blues</td>\n",
       "      <td>TRJBDVE128F9306FDB</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5827dca77aa2eb0ad91b8fdf</td>\n",
       "      <td>Bukka White</td>\n",
       "      <td>I'm taken down with the fever and it won't let...</td>\n",
       "      <td>High Fever Blues</td>\n",
       "      <td>TRRRGCS128F4280BB6</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5827dca77aa2eb0ad91b8fe2</td>\n",
       "      <td>Bukka White</td>\n",
       "      <td>Hey-eee, come on you women\\nLet's a do the the...</td>\n",
       "      <td>Bukka's Jitterbug Swing</td>\n",
       "      <td>TRXZHEC128F4280BB2</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id        artist  \\\n",
       "0  5827dca77aa2eb0ad91b8fdc   Bukka White   \n",
       "1  5827dca77aa2eb0ad91b8fdd   Bukka White   \n",
       "2  5827dca77aa2eb0ad91b8fde  Bessie Smith   \n",
       "3  5827dca77aa2eb0ad91b8fdf   Bukka White   \n",
       "4  5827dca77aa2eb0ad91b8fe2   Bukka White   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  I was over in Aberdeen\\nOn my way to New Orlea...   \n",
       "1  When a man gets trouble in his mind\\nHe wanna ...   \n",
       "2  Woke up this mornin' when chickens was crowin'...   \n",
       "3  I'm taken down with the fever and it won't let...   \n",
       "4  Hey-eee, come on you women\\nLet's a do the the...   \n",
       "\n",
       "                        title            track_id  year  \n",
       "0  Aberdeen Mississippi Blues  TRHRKYP128F4280BB1  1940  \n",
       "1            Sleepy Man Blues  TRCAHZD128F4280BC1  1940  \n",
       "2         Young Woman's Blues  TRJBDVE128F9306FDB  1940  \n",
       "3            High Fever Blues  TRRRGCS128F4280BB6  1940  \n",
       "4     Bukka's Jitterbug Swing  TRXZHEC128F4280BB2  1940  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from mongodb into pandas\n",
    "data = coll.find()\n",
    "song_lyrics = pd.DataFrame(list(data))\n",
    "song_lyrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "#from sklearn.pipeline import Pipeline\n",
    "train, test = train_test_split(song_lyrics, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.7.zip (998kB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 1.4MB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): six in /home/ubuntu/mnt/bin/anaconda3/lib/python3.5/site-packages (from langdetect)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Running setup.py bdist_wheel for langdetect ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/6f/8c/3b/ffa8151e27effd7de2a7d3194650d78fe6e4d4a3c175a74867\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.7\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.stem\n",
    "from string import punctuation\n",
    "import re\n",
    "def lyric_preprocessor(lyric, stem=False):\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    re_replace = {\n",
    "        r\"\\bdon't\\b\": \"do not\",\n",
    "        r\"\\bdoesn't\\b\": \"does not\",\n",
    "        r\"\\bdidn't\\b\": \"did not\",\n",
    "        r\"\\bhasn't\\b\": \"has not\",\n",
    "        r\"\\bhaven't\\b\": \"have not\",\n",
    "        r\"\\bhadn't\\b\": \"had not\",\n",
    "        r\"\\bwon't\\b\": \"will not\",\n",
    "        r\"\\bwouldn't\\b\": \"would not\",\n",
    "        r\"\\bcan't\\b\": \"can not\",\n",
    "        r\"\\bcannot\\b\": \"can not\",\n",
    "        r\"\\bain't\\b\": \"is not\"\n",
    "    }\n",
    "    \n",
    "    lyric = lyric.lower()\n",
    "    for r, replacement in re_replace.items():\n",
    "        lyric = re.sub(r, replacement, lyric)\n",
    "        \n",
    "    lyric_words = word_tokenize(lyric)\n",
    "    lyric_words_clean = [word for word in lyric_words if word not in stop_words]\n",
    "    if stem:\n",
    "        stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        return [stemmer.stem(word) for word in lyric_words_clean]\n",
    "    \n",
    "    # not stemmed\n",
    "    return lyric_words_clean\n",
    "\n",
    "    \n",
    "# build a scikit-learn transformer so things play nicely with sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class LyricPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, stop_words=None, lower=True, strip=True, repeat_replacer=None):\n",
    "        self.lower = lower\n",
    "        self.strip = strip\n",
    "        self.stopwords = stop_words or set(stopwords.words('english') + list(punctuation))\n",
    "        self.stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        self.repeat_replacer = repeat_replacer\n",
    "        self.re_replace = {\n",
    "            r\"\\bdon't\\b\": \"do not\",\n",
    "            r\"\\bdoesn't\\b\": \"does not\",\n",
    "            r\"\\bdidn't\\b\": \"did not\",\n",
    "            r\"\\bhasn't\\b\": \"has not\",\n",
    "            r\"\\bhaven't\\b\": \"have not\",\n",
    "            r\"\\bhadn't\\b\": \"had not\",\n",
    "            r\"\\bwon't\\b\": \"will not\",\n",
    "            r\"\\bwouldn't\\b\": \"would not\",\n",
    "            r\"\\bcan't\\b\": \"can not\",\n",
    "            r\"\\bcannot\\b\": \"can not\",\n",
    "            r\"\\bain't\\b\": \"is not\"\n",
    "            }\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.pre_process(doc)) for doc in X\n",
    "            ]\n",
    "        \n",
    "    def pre_process(self, lyric):\n",
    "        # replacements\n",
    "        for r, replacement in self.re_replace.items():\n",
    "            lyric = re.sub(r, replacement, lyric)\n",
    "        \n",
    "        for token in word_tokenize(lyric):\n",
    "            token = token.lower() if self.lower else token\n",
    "            token = token.strip() if self.strip else token\n",
    "            token = self.repeat_replacer(token) if self.repeat_replacer else token\n",
    "            if token in self.stopwords:\n",
    "                continue\n",
    "                \n",
    "            stemmed_token = self.stemmer.stem(token)\n",
    "            yield stemmed_token\n",
    "\n",
    "# normalize by removing repeating characters\n",
    "from nltk.corpus import wordnet\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word) # keep replacing\n",
    "        else:\n",
    "            return repl_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah il sait tout mon petit doigt\n",
      "Tes parties avec mes revenus\n",
      "Que d'allées venues\n",
      "\n",
      "Vers quel crayon s'est-elle taillée désormais\n",
      "Que vais-je faire de cet abandon\n",
      "À qui en faire don ?\n",
      "\n",
      "Bombez le torse bombez !\n",
      "Prenez des forces bombez !\n",
      "Bombez le torse bombez !\n",
      "Ca c'est my way\n",
      "\n",
      "I know, I know\n",
      "Sa turne a l'air habitée\n",
      "Alors qu'on sait que personne n'y vit\n",
      "À qui se fier ?\n",
      "\n",
      "Bombez le torse bombez !\n",
      "Prenez des forces bombez !\n",
      "Ca c'est my way\n",
      "\n",
      "Les paras sont normaux sous la tonnelle où rôde\n",
      "Où rôde le Japon\n",
      "Fidèle à ses traditions\n",
      "\n",
      "Dans un dernier effort\n",
      "L'empereur se soulève\n",
      "Donne à boire au dragon\n",
      "Et scrute les environs\n",
      "\n",
      "Ah l'enfant que j'ai dans le dos\n",
      "Fait se retourner tous les badauds\n",
      "Piler les autos\n",
      "\n",
      "Bombez le torse bombez !\n",
      "Prenez des forces bombez !\n",
      "Ca c'est my way\n",
      "\n",
      "ouistiti\n",
      "T'as pas souri quand elle a ri\n",
      "Tant pis\n",
      "Les alterts et les égaux\n",
      "Ca m'est égal ça m'est ego\n",
      "\n",
      "Bombez le torse bombez !\n"
     ]
    }
   ],
   "source": [
    "print(song_lyrics['lyrics'][43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ê\n",
      "Onde anda você\n",
      "Faz tempo que a gente não se vê\n",
      "Ê\n",
      "Batumaré\n",
      "É sempre bom voltar\n",
      "A luz se acendeu de novo\n",
      "A porta nunca vai\n",
      "Se fechar\n",
      "Ê\n",
      "Batumaré\n",
      "Qualquer palavra serve\n",
      "Pra dizer\n",
      "Dessa alegria\n",
      "A luz se acendeu de novo\n",
      "A porta nunca vai se fechar\n",
      "Ê\n",
      "Batumaré\n"
     ]
    }
   ],
   "source": [
    "print(song_lyrics['lyrics'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1...\\n2...\\n3...\\n4...\\n5...\\n6...\\n7...\\n8...\\n9...\\n10...\\n11...\\n12...\\n13...\\n14...\\n15...\\n16...\\n17...\\n18...\\n19...\\n20...\\n21...\\n22...\\n23...\\n24...\\n25...\\n26...\\n27...\\n28...\\n29...\\n30...\\n31...\\n32...\\n33...\\n34...\\n35...\\n36...\\n37...\\n38...\\n39'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove non-english rows\n",
    "#english_only = song_lyrics[is_english_sent(song_lyrics[['lyrics']]) ]\n",
    "#is_english_sent(song_lyrics.lyrics)\n",
    "#is_english_sent(song_lyrics.lyrics[12638])\n",
    "from langdetect import detect\n",
    "#song_lyrics[['lyrics']].apply(detect, axis=1)\n",
    "#song_lyrics[['lyrics']]\n",
    "song_lyrics.lyrics[1045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_lyrics = song_lyrics.drop(1045)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\\n21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_lyrics.lyrics[5338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_lyrics = song_lyrics.drop(5338)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_lyrics_en = song_lyrics[song_lyrics.apply(lambda x: detect(x['lyrics']) == 'en', axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id         11600\n",
       "artist      11600\n",
       "lyrics      11600\n",
       "title       11600\n",
       "track_id    11600\n",
       "year        11600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_lyrics_en.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = song_lyrics_en['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     2,     3,     4,     5,     7,     8,     9,\n",
       "               10,\n",
       "            ...\n",
       "            12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662,\n",
       "            12663],\n",
       "           dtype='int64', length=11600)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.index # some indices are missing! ouch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resetting index\n",
    "docs = docs.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create & dump vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "vectorizer = Pipeline([\n",
    "        ('preprocessor', LyricPreprocessor()),\n",
    "        ('vectorizer', TfidfVectorizer(\n",
    "            tokenizer = identity, preprocessor=None, lowercase=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dumping the vectorizer\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate & dump feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lyrics_features.pkl',\n",
       " 'lyrics_features.pkl_01.npy',\n",
       " 'lyrics_features.pkl_02.npy',\n",
       " 'lyrics_features.pkl_03.npy']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "lyrics_features = vectorizer.fit_transform(docs)\n",
    "joblib.dump(lyrics_features, 'lyrics_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial model\n",
    "num_clusters = 30\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = num_clusters, init = 'k-means++', max_iter=100, n_init=1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 19485.615\n",
      "Iteration  1, inertia 10851.962\n",
      "Iteration  2, inertia 10765.908\n",
      "Iteration  3, inertia 10724.681\n",
      "Iteration  4, inertia 10704.917\n",
      "Iteration  5, inertia 10694.394\n",
      "Iteration  6, inertia 10688.806\n",
      "Iteration  7, inertia 10684.509\n",
      "Iteration  8, inertia 10680.408\n",
      "Iteration  9, inertia 10675.986\n",
      "Iteration 10, inertia 10671.749\n",
      "Iteration 11, inertia 10667.199\n",
      "Iteration 12, inertia 10664.571\n",
      "Iteration 13, inertia 10662.201\n",
      "Iteration 14, inertia 10659.946\n",
      "Iteration 15, inertia 10658.868\n",
      "Iteration 16, inertia 10658.497\n",
      "Iteration 17, inertia 10658.319\n",
      "Iteration 18, inertia 10658.197\n",
      "Iteration 19, inertia 10658.080\n",
      "Iteration 20, inertia 10657.991\n",
      "Iteration 21, inertia 10657.904\n",
      "Iteration 22, inertia 10657.815\n",
      "Iteration 23, inertia 10657.707\n",
      "Iteration 24, inertia 10657.614\n",
      "Iteration 25, inertia 10657.526\n",
      "Iteration 26, inertia 10657.455\n",
      "Iteration 27, inertia 10657.388\n",
      "Iteration 28, inertia 10657.349\n",
      "Iteration 29, inertia 10657.302\n",
      "Iteration 30, inertia 10657.263\n",
      "Iteration 31, inertia 10657.224\n",
      "Iteration 32, inertia 10657.175\n",
      "Iteration 33, inertia 10657.133\n",
      "Iteration 34, inertia 10657.104\n",
      "Iteration 35, inertia 10657.074\n",
      "Iteration 36, inertia 10657.034\n",
      "Iteration 37, inertia 10656.994\n",
      "Iteration 38, inertia 10656.939\n",
      "Iteration 39, inertia 10656.881\n",
      "Iteration 40, inertia 10656.853\n",
      "Iteration 41, inertia 10656.802\n",
      "Iteration 42, inertia 10656.726\n",
      "Iteration 43, inertia 10656.536\n",
      "Iteration 44, inertia 10655.982\n",
      "Iteration 45, inertia 10654.726\n",
      "Iteration 46, inertia 10654.240\n",
      "Iteration 47, inertia 10654.119\n",
      "Iteration 48, inertia 10654.026\n",
      "Iteration 49, inertia 10653.986\n",
      "Converged at iteration 49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=30, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=True)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate clusters\n",
    "km.fit(lyrics_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], dtype=int32),\n",
       " array([ 207,  155,  157,  189, 1239,  274,   86,  369,  189,  600,  313,\n",
       "         308,   54,   73,  323,  217,  203,  610,  208,  130,  350, 2875,\n",
       "         167,  128,  156,  102,  259,   82, 1514,   63]))"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(km.labels_, return_counts=True) # get number of articles in each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate documents by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = {}\n",
    "for i, cluster in enumerate(km.labels_):\n",
    "    doc = docs[i]\n",
    "    if cluster not in documents.keys():\n",
    "        documents[cluster] = doc\n",
    "    else:\n",
    "        documents[cluster] += doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get words in clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'69': 1,\n",
       "          'cried': 5,\n",
       "          'loved': 13,\n",
       "          \"n't\": 621,\n",
       "          'friendstonight': 1,\n",
       "          'parted': 3,\n",
       "          'plane': 1,\n",
       "          'handed': 1,\n",
       "          'letters': 4,\n",
       "          'thing': 16,\n",
       "          'changin': 1,\n",
       "          'malo': 1,\n",
       "          'voix': 2,\n",
       "          'passing': 2,\n",
       "          'pretense': 1,\n",
       "          'patriotic': 1,\n",
       "          'everybody': 4,\n",
       "          'mat': 2,\n",
       "          'price': 1,\n",
       "          'pursuit': 1,\n",
       "          'fever': 2,\n",
       "          'lights': 7,\n",
       "          'wait': 29,\n",
       "          'deaf': 1,\n",
       "          'sweethearts': 2,\n",
       "          'bridge': 6,\n",
       "          'claimed': 1,\n",
       "          'seek': 5,\n",
       "          'nigth': 1,\n",
       "          'has': 28,\n",
       "          'down': 130,\n",
       "          'illusion': 1,\n",
       "          'conscience': 1,\n",
       "          'feelings': 10,\n",
       "          'reminding': 1,\n",
       "          'start': 72,\n",
       "          'ate': 3,\n",
       "          'driving': 2,\n",
       "          'headache': 2,\n",
       "          'fair': 2,\n",
       "          'thundering': 1,\n",
       "          'dried': 1,\n",
       "          'bullet': 2,\n",
       "          'entwine': 3,\n",
       "          'tuned': 1,\n",
       "          'shouldersi': 1,\n",
       "          'misery': 23,\n",
       "          'shape': 1,\n",
       "          'perspective': 1,\n",
       "          'forgot': 5,\n",
       "          'ta': 9,\n",
       "          'taught': 1,\n",
       "          'goes': 18,\n",
       "          'begged': 1,\n",
       "          'walk': 23,\n",
       "          'undergraddie': 1,\n",
       "          \"'cos\": 5,\n",
       "          'return': 5,\n",
       "          'tropic': 1,\n",
       "          'satisfied': 2,\n",
       "          'da-da-da': 6,\n",
       "          'sense': 2,\n",
       "          'bird': 4,\n",
       "          'she’s': 1,\n",
       "          'work': 8,\n",
       "          'celebrating': 1,\n",
       "          'doubtful': 2,\n",
       "          'impossible': 1,\n",
       "          'above': 11,\n",
       "          'mission': 3,\n",
       "          'youwhenever': 1,\n",
       "          'reflexion': 1,\n",
       "          'duties': 1,\n",
       "          'hesitate': 2,\n",
       "          'share': 3,\n",
       "          'confused': 2,\n",
       "          'tomorow': 1,\n",
       "          \"shakin'\": 1,\n",
       "          'us': 34,\n",
       "          'relive': 1,\n",
       "          'kind': 11,\n",
       "          'begun': 2,\n",
       "          'fell': 2,\n",
       "          'angels': 4,\n",
       "          'problems': 2,\n",
       "          'i’d': 1,\n",
       "          'slave': 1,\n",
       "          'weakness': 1,\n",
       "          'reasons': 2,\n",
       "          'ceiling': 1,\n",
       "          '{': 3,\n",
       "          'refuse': 1,\n",
       "          'memory': 18,\n",
       "          'lowe': 1,\n",
       "          'pieceslying': 1,\n",
       "          'made': 47,\n",
       "          'other': 26,\n",
       "          'it’s': 1,\n",
       "          'river': 8,\n",
       "          'cost': 3,\n",
       "          'on': 339,\n",
       "          'mean': 11,\n",
       "          'hello': 3,\n",
       "          'arrest': 1,\n",
       "          'trance': 2,\n",
       "          'tide': 2,\n",
       "          'glad': 5,\n",
       "          'strings': 6,\n",
       "          'stil': 1,\n",
       "          'drum': 7,\n",
       "          'cards': 4,\n",
       "          'threw': 2,\n",
       "          'smell': 1,\n",
       "          'summer': 11,\n",
       "          'heading': 1,\n",
       "          \"misery's\": 1,\n",
       "          'why': 69,\n",
       "          'bend': 1,\n",
       "          'playing': 1,\n",
       "          'american': 1,\n",
       "          'yeahanyone': 1,\n",
       "          'loser': 1,\n",
       "          'fakin': 1,\n",
       "          'harm': 2,\n",
       "          'singing': 7,\n",
       "          'numbers': 1,\n",
       "          'mind': 34,\n",
       "          'stage': 2,\n",
       "          'feet': 9,\n",
       "          'cut': 5,\n",
       "          'breaking': 64,\n",
       "          'clipped': 1,\n",
       "          'door': 20,\n",
       "          'nothin': 1,\n",
       "          'possessions': 1,\n",
       "          'wound': 1,\n",
       "          'burnt': 1,\n",
       "          'cloud': 2,\n",
       "          'heart': 1160,\n",
       "          'stop': 59,\n",
       "          'they': 89,\n",
       "          'castles': 1,\n",
       "          'room': 11,\n",
       "          'quite': 4,\n",
       "          'friends': 13,\n",
       "          'face': 22,\n",
       "          'mechanics': 3,\n",
       "          'found': 20,\n",
       "          'attention': 1,\n",
       "          'instincts': 1,\n",
       "          'talked': 1,\n",
       "          'when': 203,\n",
       "          'overcrowded': 1,\n",
       "          'unwind': 3,\n",
       "          'brow': 1,\n",
       "          'whatcha': 1,\n",
       "          'even': 17,\n",
       "          'embraces': 1,\n",
       "          'stops': 1,\n",
       "          'negativity': 1,\n",
       "          'in': 549,\n",
       "          'breathe': 2,\n",
       "          'medicine': 1,\n",
       "          'obliterating': 1,\n",
       "          'bloom': 1,\n",
       "          'look': 37,\n",
       "          'false': 2,\n",
       "          'understanding': 1,\n",
       "          'reappeared': 1,\n",
       "          'reason': 8,\n",
       "          'something': 28,\n",
       "          'loving': 25,\n",
       "          'herself': 1,\n",
       "          \"'m\": 312,\n",
       "          'lonely': 90,\n",
       "          'decisions': 1,\n",
       "          'or': 55,\n",
       "          'worst': 8,\n",
       "          'least': 4,\n",
       "          'travelin': 4,\n",
       "          'hop': 1,\n",
       "          'hugging': 2,\n",
       "          'dawn': 3,\n",
       "          'fall': 50,\n",
       "          'widedo': 1,\n",
       "          'youonly': 1,\n",
       "          'street': 11,\n",
       "          'whe': 1,\n",
       "          'hush': 1,\n",
       "          'praying': 1,\n",
       "          'embryo': 1,\n",
       "          'chorus': 19,\n",
       "          'hearts': 89,\n",
       "          'promised': 6,\n",
       "          'young': 31,\n",
       "          'happiness': 6,\n",
       "          'clue': 2,\n",
       "          'willso': 1,\n",
       "          'monument': 1,\n",
       "          'strange': 6,\n",
       "          'rainy': 3,\n",
       "          'soul': 46,\n",
       "          'gleaming': 2,\n",
       "          'throw': 7,\n",
       "          'persist': 3,\n",
       "          'crown': 1,\n",
       "          'wife': 2,\n",
       "          'rod': 1,\n",
       "          'quick': 4,\n",
       "          'hell': 9,\n",
       "          'big': 17,\n",
       "          'shaking': 4,\n",
       "          'his': 64,\n",
       "          'suppers': 1,\n",
       "          'saidliving': 1,\n",
       "          'my': 994,\n",
       "          'better': 54,\n",
       "          'sad': 8,\n",
       "          'window': 23,\n",
       "          'mend': 9,\n",
       "          'shadows': 5,\n",
       "          'diary': 1,\n",
       "          'risk': 3,\n",
       "          'treating': 2,\n",
       "          'while': 28,\n",
       "          'jim': 1,\n",
       "          'sit': 9,\n",
       "          'measures': 1,\n",
       "          'knocked': 1,\n",
       "          'hangin': 1,\n",
       "          'tune': 2,\n",
       "          'superstar': 6,\n",
       "          'head': 24,\n",
       "          'satisfy': 1,\n",
       "          'surely': 8,\n",
       "          'notion': 4,\n",
       "          \"n'soul\": 1,\n",
       "          'hearta': 1,\n",
       "          'ahh': 1,\n",
       "          'till': 15,\n",
       "          'small': 3,\n",
       "          'moonlight': 2,\n",
       "          'more': 61,\n",
       "          'faith': 3,\n",
       "          'sights': 1,\n",
       "          'losin': 2,\n",
       "          'cry': 49,\n",
       "          'lover': 20,\n",
       "          'monsters': 1,\n",
       "          'twas': 1,\n",
       "          'clouds': 2,\n",
       "          'history': 4,\n",
       "          'weak': 11,\n",
       "          'goin': 1,\n",
       "          'loves': 17,\n",
       "          \"'ll\": 226,\n",
       "          'cars': 1,\n",
       "          'falling': 13,\n",
       "          'wake': 7,\n",
       "          'limb': 1,\n",
       "          \"'s\": 523,\n",
       "          'insincere': 1,\n",
       "          '--': 3,\n",
       "          'survey': 1,\n",
       "          'flowers': 1,\n",
       "          'hurry': 1,\n",
       "          'boyfriend': 1,\n",
       "          'inhibitions': 1,\n",
       "          'skip': 3,\n",
       "          'longstanding': 1,\n",
       "          'sacrifice': 2,\n",
       "          'affection': 3,\n",
       "          'freedom': 2,\n",
       "          'hate': 3,\n",
       "          'carry': 3,\n",
       "          'mystery': 3,\n",
       "          'dithering': 1,\n",
       "          'haddie': 1,\n",
       "          'explanations': 1,\n",
       "          'dough': 1,\n",
       "          'drugs': 1,\n",
       "          'heartwhen': 1,\n",
       "          'secret': 21,\n",
       "          'yeahnow': 1,\n",
       "          'split': 3,\n",
       "          'austen': 1,\n",
       "          'separate': 2,\n",
       "          'own': 22,\n",
       "          'complain': 1,\n",
       "          'uttered': 1,\n",
       "          'denied': 1,\n",
       "          'hear': 41,\n",
       "          'begin': 8,\n",
       "          'works': 2,\n",
       "          'know': 253,\n",
       "          'myself': 37,\n",
       "          'weary': 1,\n",
       "          'but': 284,\n",
       "          'said': 53,\n",
       "          'star': 5,\n",
       "          'greetin': 1,\n",
       "          'subway': 4,\n",
       "          'plunge': 1,\n",
       "          'dreams': 19,\n",
       "          'wishing': 2,\n",
       "          'boy': 27,\n",
       "          'clock': 1,\n",
       "          'staring': 1,\n",
       "          'our': 36,\n",
       "          'tells': 4,\n",
       "          'children': 3,\n",
       "          'repeats': 1,\n",
       "          'bowed': 1,\n",
       "          'smoke': 1,\n",
       "          'stolen': 2,\n",
       "          'welch': 1,\n",
       "          'watch': 2,\n",
       "          'collapse': 1,\n",
       "          'the': 1188,\n",
       "          'gems': 1,\n",
       "          'lives': 3,\n",
       "          'pages': 1,\n",
       "          'lonelier': 1,\n",
       "          'city': 13,\n",
       "          'rely': 1,\n",
       "          'wall': 18,\n",
       "          'overflow': 1,\n",
       "          'ho': 2,\n",
       "          'cant': 3,\n",
       "          'heartbreak': 3,\n",
       "          'ouuuuui': 1,\n",
       "          'jewel': 1,\n",
       "          'sore': 1,\n",
       "          'catch': 6,\n",
       "          'dwell': 1,\n",
       "          'twice': 2,\n",
       "          'explain': 4,\n",
       "          'bag': 2,\n",
       "          'ill': 1,\n",
       "          'anyone': 29,\n",
       "          'ride': 2,\n",
       "          'vain': 2,\n",
       "          'farther': 1,\n",
       "          'outside': 5,\n",
       "          'ma': 1,\n",
       "          'changing': 3,\n",
       "          'entwined': 1,\n",
       "          'shoulder': 7,\n",
       "          'silent': 1,\n",
       "          'kissall': 1,\n",
       "          'accounts': 1,\n",
       "          'suitcase': 2,\n",
       "          'heat': 1,\n",
       "          'steppin': 1,\n",
       "          'slightest': 1,\n",
       "          'plans': 1,\n",
       "          'fickle': 1,\n",
       "          'glances': 1,\n",
       "          'bringing': 2,\n",
       "          'hurt': 33,\n",
       "          'green': 2,\n",
       "          'ball': 1,\n",
       "          'means': 2,\n",
       "          'wish': 25,\n",
       "          'slip': 2,\n",
       "          'april': 2,\n",
       "          'save': 1,\n",
       "          'runs': 2,\n",
       "          'receive': 2,\n",
       "          'listen': 2,\n",
       "          'schemes': 1,\n",
       "          'mask': 1,\n",
       "          'photo': 1,\n",
       "          'ringing': 1,\n",
       "          'tall': 2,\n",
       "          'treat': 3,\n",
       "          'sweating': 2,\n",
       "          \"ev'rything\": 1,\n",
       "          'six': 2,\n",
       "          'voices': 1,\n",
       "          'toy': 2,\n",
       "          'lent': 1,\n",
       "          'mineoh': 1,\n",
       "          'husband': 1,\n",
       "          'implied': 1,\n",
       "          'reading': 2,\n",
       "          'moons': 2,\n",
       "          'who': 67,\n",
       "          'everytime': 3,\n",
       "          'men': 13,\n",
       "          'keep': 43,\n",
       "          'hookwell': 1,\n",
       "          'game': 19,\n",
       "          \"'lectric\": 1,\n",
       "          'amen': 1,\n",
       "          'longing': 5,\n",
       "          'going': 8,\n",
       "          'wander': 2,\n",
       "          'sends': 2,\n",
       "          'blanc.ca': 1,\n",
       "          'sip': 1,\n",
       "          'minute': 1,\n",
       "          'flown': 1,\n",
       "          'kept': 4,\n",
       "          'objects': 1,\n",
       "          'aid': 1,\n",
       "          'them': 36,\n",
       "          'called': 2,\n",
       "          'owwwww': 1,\n",
       "          'someone': 35,\n",
       "          'self': 1,\n",
       "          'handle': 1,\n",
       "          'attraction': 2,\n",
       "          \"'cuz\": 2,\n",
       "          'crappy': 1,\n",
       "          'hundred': 4,\n",
       "          'earn': 1,\n",
       "          'blind': 7,\n",
       "          'light': 25,\n",
       "          'using': 1,\n",
       "          'arabian': 1,\n",
       "          'grass': 1,\n",
       "          'this': 216,\n",
       "          'singin': 1,\n",
       "          'echoes': 1,\n",
       "          'let': 148,\n",
       "          'rough': 1,\n",
       "          'canyons': 4,\n",
       "          'keepin': 2,\n",
       "          'picture': 5,\n",
       "          'point': 3,\n",
       "          'stronger': 2,\n",
       "          'bet': 4,\n",
       "          'crawl': 3,\n",
       "          'rockin': 1,\n",
       "          'pillow': 5,\n",
       "          'mad': 1,\n",
       "          'confusion': 3,\n",
       "          'incinerate': 1,\n",
       "          'plan': 1,\n",
       "          'weight': 4,\n",
       "          'glorious': 1,\n",
       "          'mink': 1,\n",
       "          'soulit': 1,\n",
       "          'refusin': 3,\n",
       "          'settled': 1,\n",
       "          'feelin': 4,\n",
       "          'helping': 1,\n",
       "          'fact': 7,\n",
       "          'stumbles': 1,\n",
       "          'rock': 6,\n",
       "          'whirlwind': 2,\n",
       "          'wellyou': 1,\n",
       "          'shallow': 1,\n",
       "          'scene': 1,\n",
       "          'lie': 12,\n",
       "          'over': 35,\n",
       "          'cliches': 1,\n",
       "          'single': 3,\n",
       "          'shining': 3,\n",
       "          'check': 1,\n",
       "          'met': 11,\n",
       "          'intentions': 2,\n",
       "          'tangle': 4,\n",
       "          'tender': 10,\n",
       "          \"doesn't\": 1,\n",
       "          'hauntin': 1,\n",
       "          'place': 12,\n",
       "          'demand': 1,\n",
       "          'song': 19,\n",
       "          'excuse': 1,\n",
       "          '/': 3,\n",
       "          'design': 2,\n",
       "          'plate': 1,\n",
       "          'riding': 1,\n",
       "          'fits': 3,\n",
       "          'dreamers': 1,\n",
       "          'nine': 2,\n",
       "          'played': 5,\n",
       "          'crushing': 1,\n",
       "          'older': 7,\n",
       "          'else': 12,\n",
       "          'reelin': 1,\n",
       "          'wedding': 1,\n",
       "          'heartit': 1,\n",
       "          'againlies': 1,\n",
       "          'tainted': 2,\n",
       "          'unto': 1,\n",
       "          'matches': 1,\n",
       "          'boundless': 1,\n",
       "          'starless': 1,\n",
       "          'x2': 1,\n",
       "          'cuties': 1,\n",
       "          \"lookin'\": 1,\n",
       "          'true.right': 1,\n",
       "          'sometimes': 13,\n",
       "          'proved': 1,\n",
       "          'tied': 8,\n",
       "          'glow': 2,\n",
       "          'dance': 2,\n",
       "          'missionary': 1,\n",
       "          '‘fore': 1,\n",
       "          'tooth': 1,\n",
       "          'resist': 3,\n",
       "          'wino': 1,\n",
       "          'lean': 3,\n",
       "          'incomplete': 2,\n",
       "          'miss': 14,\n",
       "          'edit': 1,\n",
       "          'then': 65,\n",
       "          'acting': 1,\n",
       "          'swallow': 1,\n",
       "          'de': 1,\n",
       "          'clean': 2,\n",
       "          'man': 36,\n",
       "          'glowing': 1,\n",
       "          'pane': 3,\n",
       "          'of': 510,\n",
       "          'throat': 1,\n",
       "          \"'round\": 2,\n",
       "          'hard': 33,\n",
       "          'tries': 1,\n",
       "          'feeling': 13,\n",
       "          'damage': 6,\n",
       "          'youooo': 1,\n",
       "          'hint': 1,\n",
       "          'perhaps': 1,\n",
       "          'off': 29,\n",
       "          'storm': 2,\n",
       "          'repeating': 4,\n",
       "          'harbor': 1,\n",
       "          \"tryin'\": 1,\n",
       "          'little': 91,\n",
       "          'distance': 3,\n",
       "          'land': 3,\n",
       "          'bluethere': 1,\n",
       "          'loveless': 1,\n",
       "          'devours': 1,\n",
       "          'blue': 25,\n",
       "          'photography': 1,\n",
       "          'brighter': 1,\n",
       "          'splendor': 3,\n",
       "          'stood': 9,\n",
       "          'come': 151,\n",
       "          'torn': 1,\n",
       "          'married': 3,\n",
       "          'surrounded': 1,\n",
       "          'whom': 2,\n",
       "          'boom': 1,\n",
       "          'say': 105,\n",
       "          'answered': 2,\n",
       "          'repeat': 7,\n",
       "          'stand': 17,\n",
       "          'wrap': 2,\n",
       "          'heartall': 1,\n",
       "          'shiver': 1,\n",
       "          'joys': 2,\n",
       "          'several': 2,\n",
       "          'forgiving': 1,\n",
       "          'twocar': 1,\n",
       "          'fooled': 2,\n",
       "          'radio': 1,\n",
       "          'weather': 3,\n",
       "          'penny': 1,\n",
       "          'anything': 5,\n",
       "          'familiar': 3,\n",
       "          'happiest': 1,\n",
       "          'need': 39,\n",
       "          'true': 52,\n",
       "          'ever': 37,\n",
       "          'gazed': 1,\n",
       "          'darkness': 4,\n",
       "          'violins': 1,\n",
       "          'sucker': 1,\n",
       "          'communities': 3,\n",
       "          'pity': 2,\n",
       "          'circle': 1,\n",
       "          'whole': 11,\n",
       "          ':': 23,\n",
       "          'stumble': 1,\n",
       "          'knights': 1,\n",
       "          'heal': 2,\n",
       "          'earnest': 1,\n",
       "          'mistake': 10,\n",
       "          'teardrops': 4,\n",
       "          'at': 100,\n",
       "          'back': 89,\n",
       "          'pass': 7,\n",
       "          'fine': 15,\n",
       "          'clear': 9,\n",
       "          'ordered': 1,\n",
       "          'takes': 19,\n",
       "          'ocean': 3,\n",
       "          'path': 1,\n",
       "          'crying': 3,\n",
       "          'become': 3,\n",
       "          'embrace': 3,\n",
       "          'away': 80,\n",
       "          'hated': 1,\n",
       "          'afternoon': 2,\n",
       "          'together': 8,\n",
       "          'whose': 1,\n",
       "          'suggested': 1,\n",
       "          'impotent': 1,\n",
       "          'money': 4,\n",
       "          \"don'cha\": 1,\n",
       "          'funny': 1,\n",
       "          'wheel': 1,\n",
       "          'looking': 29,\n",
       "          'heart.there': 1,\n",
       "          'highway': 2,\n",
       "          'coin': 1,\n",
       "          'changes': 2,\n",
       "          'gold': 5,\n",
       "          'pay': 8,\n",
       "          'jubilation': 2,\n",
       "          'gray': 1,\n",
       "          'jealousy': 2,\n",
       "          'shroud': 1,\n",
       "          'howling': 1,\n",
       "          'am': 26,\n",
       "          'lock': 3,\n",
       "          'causin': 1,\n",
       "          'pray': 9,\n",
       "          'gets': 2,\n",
       "          'football': 1,\n",
       "          'ambition': 2,\n",
       "          'cool': 10,\n",
       "          'whymetcalfe-noble': 1,\n",
       "          'brand': 1,\n",
       "          'one': 193,\n",
       "          'conversations': 1,\n",
       "          'death': 1,\n",
       "          'mewhen': 1,\n",
       "          'disappeared': 1,\n",
       "          'patriot': 8,\n",
       "          'immune': 1,\n",
       "          'chime': 1,\n",
       "          'sells': 2,\n",
       "          'run': 12,\n",
       "          'perfect': 3,\n",
       "          'tonightlead': 1,\n",
       "          'wrong': 30,\n",
       "          'knowing': 18,\n",
       "          'striking': 1,\n",
       "          'everlasting': 1,\n",
       "          'wings': 5,\n",
       "          'defeat': 1,\n",
       "          'missus': 1,\n",
       "          'goodhow': 1,\n",
       "          'ruined': 1,\n",
       "          'a': 817,\n",
       "          \"wasn't\": 1,\n",
       "          'givin': 1,\n",
       "          'crack': 4,\n",
       "          'stock': 1,\n",
       "          'coz': 3,\n",
       "          'behind': 12,\n",
       "          'imprisoned': 2,\n",
       "          'self-defense': 1,\n",
       "          'aaah': 4,\n",
       "          'responsibility': 2,\n",
       "          'left': 23,\n",
       "          'covering': 3,\n",
       "          '3x': 4,\n",
       "          'dearopen': 1,\n",
       "          'treasured': 2,\n",
       "          'onewell': 1,\n",
       "          'vocals': 1,\n",
       "          'think': 65,\n",
       "          'wretched': 1,\n",
       "          'job': 1,\n",
       "          'machine': 1,\n",
       "          'exist': 1,\n",
       "          'covers': 1,\n",
       "          'stealin': 1,\n",
       "          'really': 36,\n",
       "          'stripper': 1,\n",
       "          'gladly': 3,\n",
       "          'drift': 2,\n",
       "          'guy': 8,\n",
       "          'carelessly': 1,\n",
       "          'heh': 1,\n",
       "          'lately': 1,\n",
       "          'play': 11,\n",
       "          'wasted': 2,\n",
       "          'thoughts': 6,\n",
       "          'losing': 4,\n",
       "          'way': 85,\n",
       "          'bones': 1,\n",
       "          'forms': 1,\n",
       "          'cities': 1,\n",
       "          'fellow': 2,\n",
       "          'robin': 1,\n",
       "          'so': 261,\n",
       "          'wants': 3,\n",
       "          'raise': 1,\n",
       "          'rising': 1,\n",
       "          'island': 5,\n",
       "          'search': 1,\n",
       "          'unawares': 1,\n",
       "          'ought': 7,\n",
       "          'extends': 1,\n",
       "          'destiny': 1,\n",
       "          'delight': 1,\n",
       "          'valley': 1,\n",
       "          'ripped': 1,\n",
       "          'primitive': 8,\n",
       "          'drummy': 1,\n",
       "          'crates': 1,\n",
       "          'novel': 1,\n",
       "          'eager': 3,\n",
       "          'melting': 1,\n",
       "          'rather': 6,\n",
       "          'rid': 1,\n",
       "          'countless': 1,\n",
       "          'yoga': 2,\n",
       "          'keeping': 1,\n",
       "          'heartwish': 1,\n",
       "          'easily': 1,\n",
       "          'darkest': 1,\n",
       "          'throbbing': 1,\n",
       "          'lovin': 2,\n",
       "          'old': 51,\n",
       "          'heartfools': 2,\n",
       "          'fixed': 1,\n",
       "          'time': 126,\n",
       "          'teach': 4,\n",
       "          'spark': 4,\n",
       "          'dating': 1,\n",
       "          'eyes': 44,\n",
       "          'proximity': 1,\n",
       "          'raging': 1,\n",
       "          'filthy': 2,\n",
       "          'spare': 1,\n",
       "          'desert': 2,\n",
       "          'bought': 2,\n",
       "          'tease': 1,\n",
       "          'wiser': 1,\n",
       "          'thinkin': 2,\n",
       "          'without': 36,\n",
       "          'plain': 2,\n",
       "          'bounce': 1,\n",
       "          'that': 493,\n",
       "          'wanted': 9,\n",
       "          'hard-swallowed': 1,\n",
       "          'movin': 1,\n",
       "          'disagree': 1,\n",
       "          'week': 4,\n",
       "          'constant': 1,\n",
       "          'electric': 1,\n",
       "          'help': 13,\n",
       "          \"achin'\": 1,\n",
       "          'heartache': 5,\n",
       "          'worked': 1,\n",
       "          'popular': 3,\n",
       "          'toolove': 1,\n",
       "          'timemy': 1,\n",
       "          'chain': 3,\n",
       "          'sunlight': 3,\n",
       "          'wine': 3,\n",
       "          'fire': 26,\n",
       "          'dry': 1,\n",
       "          'emerge': 3,\n",
       "          'confess': 2,\n",
       "          'strong': 18,\n",
       "          'number': 1,\n",
       "          'flames': 4,\n",
       "          'solve': 2,\n",
       "          'landslide': 1,\n",
       "          'see': 96,\n",
       "          'with': 220,\n",
       "          'waited': 3,\n",
       "          'physical': 3,\n",
       "          'wide': 4,\n",
       "          'there': 153,\n",
       "          'againi': 1,\n",
       "          'controller': 1,\n",
       "          'killing': 1,\n",
       "          'danger': 3,\n",
       "          'revenge': 2,\n",
       "          'oh': 118,\n",
       "          'blood': 8,\n",
       "          'assistant': 2,\n",
       "          'nearly': 2,\n",
       "          'known': 21,\n",
       "          'sky': 12,\n",
       "          'spinning': 1,\n",
       "          'crush': 1,\n",
       "          'anyway': 3,\n",
       "          'ground': 9,\n",
       "          'anymore': 18,\n",
       "          'whenever': 4,\n",
       "          'became': 2,\n",
       "          'dancing': 2,\n",
       "          'cos': 1,\n",
       "          \"breakin'\": 2,\n",
       "          'tempo': 1,\n",
       "          'suppose': 2,\n",
       "          'course': 3,\n",
       "          'hospital': 2,\n",
       "          'deny': 1,\n",
       "          '!': 57,\n",
       "          'does': 18,\n",
       "          'harsh': 1,\n",
       "          'rotten': 1,\n",
       "          'moments': 1,\n",
       "          'youth': 2,\n",
       "          'sort': 1,\n",
       "          'tell': 138,\n",
       "          '?': 129,\n",
       "          'tactfulness': 1,\n",
       "          'kill': 1,\n",
       "          'laugh': 4,\n",
       "          'seein': 2,\n",
       "          'imagine': 1,\n",
       "          'destruction': 1,\n",
       "          'neat': 1,\n",
       "          'first': 11,\n",
       "          'reach': 7,\n",
       "          'checkin': 2,\n",
       "          'next': 5,\n",
       "          'looks': 5,\n",
       "          'do': 434,\n",
       "          'things': 41,\n",
       "          'greater': 1,\n",
       "          'discover': 2,\n",
       "          'good': 50,\n",
       "          'gettin': 1,\n",
       "          'alright': 2,\n",
       "          'gave': 17,\n",
       "          'scalpel': 1,\n",
       "          'repossess': 3,\n",
       "          'sees': 1,\n",
       "          'arm': 1,\n",
       "          'ring': 7,\n",
       "          'heartgoldsboro': 1,\n",
       "          'captain': 1,\n",
       "          'command': 3,\n",
       "          'spring': 3,\n",
       "          'selling': 1,\n",
       "          \"turnin'\": 1,\n",
       "          'refuge': 1,\n",
       "          'amber': 1,\n",
       "          'vows': 1,\n",
       "          'whistle': 1,\n",
       "          'pine': 2,\n",
       "          'had': 92,\n",
       "          'dinner': 1,\n",
       "          'what': 151,\n",
       "          'nice': 2,\n",
       "          'unless': 2,\n",
       "          'sadly': 1,\n",
       "          'yet': 8,\n",
       "          'shunned': 2,\n",
       "          'chest': 1,\n",
       "          'piercing': 1,\n",
       "          'planes': 2,\n",
       "          'chance': 27,\n",
       "          'passion': 3,\n",
       "          'quits': 1,\n",
       "          'moon': 18,\n",
       "          'predicament': 1,\n",
       "          'illusions': 1,\n",
       "          'jukebox': 1,\n",
       "          'conditioned': 1,\n",
       "          'san': 5,\n",
       "          'trem': 1,\n",
       "          'heard': 11,\n",
       "          'shoes': 4,\n",
       "          'weddin': 1,\n",
       "          'trusting': 3,\n",
       "          'drunk': 1,\n",
       "          'leave': 43,\n",
       "          'whatever': 8,\n",
       "          'tales': 2,\n",
       "          'tenderly': 1,\n",
       "          'to': 915,\n",
       "          'fighter': 1,\n",
       "          'fairly': 2,\n",
       "          'againthe': 1,\n",
       "          'pastime': 1,\n",
       "          'felt': 20,\n",
       "          'masquerade': 1,\n",
       "          'being': 14,\n",
       "          'breath': 5,\n",
       "          'cardiac': 1,\n",
       "          'doubt': 3,\n",
       "          'west': 8,\n",
       "          'count': 3,\n",
       "          'minds': 2,\n",
       "          'hearse': 1,\n",
       "          'deeper': 3,\n",
       "          'divine': 1,\n",
       "          'changed': 2,\n",
       "          'breakin': 10,\n",
       "          'mountain': 1,\n",
       "          'stillturn': 1,\n",
       "          ']': 7,\n",
       "          'blow': 2,\n",
       "          'rain': 11,\n",
       "          'escapes': 2,\n",
       "          'stayed': 1,\n",
       "          'mirror': 4,\n",
       "          'give': 65,\n",
       "          'youdid': 1,\n",
       "          '..love': 1,\n",
       "          'painfully': 1,\n",
       "          'loss': 2,\n",
       "          'their': 20,\n",
       "          'anytime': 1,\n",
       "          'school': 1,\n",
       "          'lttle': 1,\n",
       "          'yesterdays': 1,\n",
       "          'quit': 1,\n",
       "          'somewhere': 5,\n",
       "          'few': 2,\n",
       "          'broken': 51,\n",
       "          'greatest': 4,\n",
       "          'won’t': 2,\n",
       "          'enchantment': 1,\n",
       "          'daylight': 2,\n",
       "          'covered': 4,\n",
       "          'crew': 1,\n",
       "          'phony': 1,\n",
       "          'til': 2,\n",
       "          'grow': 8,\n",
       "          'laddie': 2,\n",
       "          'kamasutra': 1,\n",
       "          'weels': 1,\n",
       "          'shout': 26,\n",
       "          'sendin': 3,\n",
       "          'hurts': 7,\n",
       "          'explode': 1,\n",
       "          'woman': 14,\n",
       "          'shudder': 1,\n",
       "          'goodnightif': 1,\n",
       "          'breeze': 3,\n",
       "          'b-b-b-bangin': 4,\n",
       "          'moves': 1,\n",
       "          'showed': 2,\n",
       "          'rings': 1,\n",
       "          'concentrated': 1,\n",
       "          \"c'mon\": 5,\n",
       "          '...': 40,\n",
       "          'important': 1,\n",
       "          'unassisted': 1,\n",
       "          'brings': 1,\n",
       "          'unsaid': 1,\n",
       "          'lunch': 2,\n",
       "          \"weren't\": 1,\n",
       "          'plato': 1,\n",
       "          'permission': 1,\n",
       "          'willingly': 2,\n",
       "          'prescription': 1,\n",
       "          'lust': 1,\n",
       "          'mirrors': 1,\n",
       "          'happened': 1,\n",
       "          'lines': 2,\n",
       "          'distressed': 1,\n",
       "          'shine': 10,\n",
       "          'sorry': 7,\n",
       "          'goodbye': 22,\n",
       "          'bitter': 7,\n",
       "          'walls': 2,\n",
       "          'special': 1,\n",
       "          'betting': 2,\n",
       "          \"'em\": 6,\n",
       "          'ca': 138,\n",
       "          'bore': 2,\n",
       "          'loan': 9,\n",
       "          'pal': 1,\n",
       "          'finish': 1,\n",
       "          'derive': 2,\n",
       "          'heartmaybe': 1,\n",
       "          'aggravation': 1,\n",
       "          'cries': 7,\n",
       "          'impatient': 1,\n",
       "          'platter': 1,\n",
       "          'someday': 21,\n",
       "          'making': 13,\n",
       "          'blows': 1,\n",
       "          'empty': 8,\n",
       "          'gentle': 6,\n",
       "          'victory': 1,\n",
       "          \"'bout\": 1,\n",
       "          'tear': 11,\n",
       "          'stay': 21,\n",
       "          'raining': 15,\n",
       "          'defers': 1,\n",
       "          'sweeter': 2,\n",
       "          'italian': 2,\n",
       "          'some': 44,\n",
       "          'lovely': 5,\n",
       "          'passin': 1,\n",
       "          'indifference': 1,\n",
       "          'cobwebs': 1,\n",
       "          'woke': 2,\n",
       "          'did': 57,\n",
       "          'her': 80,\n",
       "          ...})"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## analyze words from each cluster (topics)\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "word_sent = word_tokenize(documents[0].lower())\n",
    "word_sent = [word for word in word_sent]\n",
    "freq = FreqDist(word_sent)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'you',\n",
       " ',',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'my',\n",
       " 'and',\n",
       " 'to',\n",
       " 'a',\n",
       " 'it',\n",
       " 'me',\n",
       " \"n't\",\n",
       " 'in',\n",
       " 'love',\n",
       " 'your',\n",
       " \"'s\",\n",
       " 'of',\n",
       " 'that',\n",
       " 'do',\n",
       " 'on',\n",
       " \"'m\",\n",
       " 'be',\n",
       " 'for',\n",
       " 'but',\n",
       " 'so',\n",
       " 'break',\n",
       " 'all',\n",
       " 'know',\n",
       " 'is',\n",
       " 'can',\n",
       " \"'ll\",\n",
       " '.',\n",
       " 'just',\n",
       " 'with',\n",
       " 'this',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'when',\n",
       " 'if',\n",
       " 'one',\n",
       " \"'ve\",\n",
       " 'take',\n",
       " ')',\n",
       " '(',\n",
       " \"'\",\n",
       " 'never',\n",
       " 'no',\n",
       " 'like',\n",
       " 'now',\n",
       " 'there',\n",
       " 'come',\n",
       " 'what',\n",
       " 'up',\n",
       " 'let',\n",
       " 'only',\n",
       " 'will',\n",
       " 'was',\n",
       " 'have',\n",
       " 'ca',\n",
       " 'tell',\n",
       " 'are',\n",
       " 'he',\n",
       " 'down',\n",
       " '?',\n",
       " 'got',\n",
       " 'time',\n",
       " 'oh',\n",
       " 'go',\n",
       " 'she',\n",
       " 'not',\n",
       " 'out',\n",
       " 'could',\n",
       " 'say',\n",
       " 'from',\n",
       " 'mine',\n",
       " 'at',\n",
       " 'baby',\n",
       " 'make',\n",
       " 'see',\n",
       " 'want',\n",
       " 'feel',\n",
       " 'as',\n",
       " 'how',\n",
       " 'wo',\n",
       " 'had',\n",
       " 'again',\n",
       " 'little',\n",
       " 'lonely',\n",
       " 'they',\n",
       " 'hearts',\n",
       " 'back',\n",
       " 'would',\n",
       " \"'d\",\n",
       " 'way',\n",
       " 'where',\n",
       " 'away',\n",
       " 'her',\n",
       " 'been',\n",
       " 'still',\n",
       " 'too']"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlargest(100, freq, key=freq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## analyze words from each cluster (topics)\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "\n",
    "stp_words = set(stopwords.words('english') + list(punctuation))\n",
    "keywords = {}\n",
    "counts = {}\n",
    "for cluster in range(num_clusters):\n",
    "    word_sent = word_tokenize(documents[cluster].lower())\n",
    "    word_sent = [word for word in word_sent if word not in stp_words] # all words in the cluster\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100, freq, key=freq.get)\n",
    "    counts[cluster] = freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get unique words from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(range(10)) - set([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### get unique keywords from each cluster\n",
    "themes = {}\n",
    "for cluster in range(num_clusters):\n",
    "    other = list(set(range(10)) - set([cluster]))\n",
    "    keywords_other = set([])\n",
    "    for idx, oc in enumerate(other):\n",
    "        keywords_other.union( set(keywords[other[idx]]))\n",
    "    unique = set(keywords[cluster]) - keywords_other\n",
    "    themes[cluster] = nlargest(15, unique, key=counts[cluster].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster:  0\n",
      "theme:  ['heart', \"n't\", 'love', \"'s\", \"'m\", 'break', 'know', \"'ll\", \"'re\", 'one', 'take', \"'ve\", 'never', 'like', 'come']\n",
      "\n",
      "\n",
      "cluster:  1\n",
      "theme:  ['sun', \"'s\", \"n't\", 'shine', 'let', \"'m\", 'see', 'know', \"'re\", 'like', 'love', 'come', 'light', \"'ll\", 'day']\n",
      "\n",
      "\n",
      "cluster:  2\n",
      "theme:  ['na', 'wan', \"n't\", \"'s\", 'know', 'love', \"'m\", 'baby', 'go', \"'re\", 'get', 'want', \"'ll\", 'like', 'see']\n",
      "\n",
      "\n",
      "cluster:  3\n",
      "theme:  ['...', \"'s\", \"n't\", 'love', \"'m\", 'know', 'oh', 'never', 'baby', 'yeah', \"'re\", \"'ll\", 'like', 'go', 'got']\n",
      "\n",
      "\n",
      "cluster:  4\n",
      "theme:  [\"n't\", \"'s\", \"'m\", \"'ll\", 'know', \"'re\", 'never', \"'ve\", 'love', 'one', 'like', 'see', 'go', 'away', 'say']\n",
      "\n",
      "\n",
      "cluster:  5\n",
      "theme:  [\"''\", \"'s\", '``', \"n't\", 'said', \"'m\", \"'ll\", 'got', 'know', 'say', 'like', \"'re\", 'one', 'go', 'back']\n",
      "\n",
      "\n",
      "cluster:  6\n",
      "theme:  ['shake', 'train', \"n't\", \"'s\", 'baby', \"'m\", 'get', 'got', 'right', 'know', 'see', 'home', \"'em\", 'time', 'na']\n",
      "\n",
      "\n",
      "cluster:  7\n",
      "theme:  ['got', \"n't\", \"'s\", \"'ve\", 'ta', \"'m\", 'know', 'get', 'love', \"'re\", 'go', 'like', \"'ll\", 'time', 'oh']\n",
      "\n",
      "\n",
      "cluster:  8\n",
      "theme:  ['want', \"n't\", \"'s\", 'love', 'know', \"'m\", 'go', 'need', \"'re\", 'see', \"'ll\", 'got', 'like', 'oh', 'get']\n",
      "\n",
      "\n",
      "cluster:  9\n",
      "theme:  [\"'s\", \"n't\", 'know', \"'re\", \"'m\", 'love', 'like', 'got', 'way', \"'ll\", 'one', 'let', 'ca', 'right', 'get']\n",
      "\n",
      "\n",
      "cluster:  10\n",
      "theme:  ['na', 'gon', \"'m\", \"'s\", \"n't\", \"'re\", 'get', 'love', 'got', 'baby', 'know', 'oh', 'time', \"'ll\", 'make']\n",
      "\n",
      "\n",
      "cluster:  11\n",
      "theme:  [\"n't\", 'make', \"'s\", 'know', 'try', \"'ll\", 'love', \"'m\", 'feel', 'ca', 'cry', 'like', \"'re\", 'go', 'get']\n",
      "\n",
      "\n",
      "cluster:  12\n",
      "theme:  ['angel', \"'s\", \"n't\", 'love', 'angels', 'heaven', 'eyes', 'baby', 'know', \"'re\", 'little', 'like', \"'m\", 'see', 'ai']\n",
      "\n",
      "\n",
      "cluster:  13\n",
      "theme:  ['rock', 'na', \"'s\", \"n't\", 'roll', 'gon', \"'re\", \"'m\", 'baby', 'got', 'like', 'let', 'yeah', 'well', 'wan']\n",
      "\n",
      "\n",
      "cluster:  14\n",
      "theme:  ['life', \"'s\", \"n't\", \"'m\", 'one', 'time', 'see', 'love', 'know', 'death', \"'re\", \"'ve\", \"'ll\", 'god', 'come']\n",
      "\n",
      "\n",
      "cluster:  15\n",
      "theme:  ['yeah', \"'s\", \"n't\", 'oh', 'love', \"'m\", 'know', 'got', 'baby', 'get', \"'re\", 'come', \"'ve\", 'ooh', 'feel']\n",
      "\n",
      "\n",
      "cluster:  16\n",
      "theme:  ['think', \"n't\", \"'s\", \"'m\", 'know', \"'re\", \"'ll\", 'go', 'love', \"'ve\", 'like', 'day', 'got', 'thinking', 'see']\n",
      "\n",
      "\n",
      "cluster:  17\n",
      "theme:  ['love', \"n't\", \"'s\", \"'m\", 'know', \"'ll\", 'oh', \"'ve\", \"'re\", 'baby', 'never', 'need', 'like', 'let', 'say']\n",
      "\n",
      "\n",
      "cluster:  18\n",
      "theme:  ['time', \"'s\", \"n't\", \"'ll\", 'times', 'know', 'love', \"'m\", 'long', \"'ve\", \"'re\", 'way', 'like', 'day', 'could']\n",
      "\n",
      "\n",
      "cluster:  19\n",
      "theme:  ['dream', 'dreams', \"'s\", \"'m\", \"n't\", 'dreaming', 'love', 'know', 'like', \"'re\", 'see', 'christmas', \"'ll\", 'come', 'one']\n",
      "\n",
      "\n",
      "cluster:  20\n",
      "theme:  ['baby', \"n't\", \"'s\", 'love', \"'m\", 'come', 'know', 'oh', 'got', \"'re\", 'back', 'yeah', \"'ll\", 'let', 'like']\n",
      "\n",
      "\n",
      "cluster:  21\n",
      "theme:  [\"'s\", \"n't\", 'love', \"'re\", 'like', 'oh', \"'m\", \"'ll\", 'see', 'know', 'come', 'one', 'go', 'ca', 'time']\n",
      "\n",
      "\n",
      "cluster:  22\n",
      "theme:  [\"n't\", 'live', \"'s\", 'living', \"'m\", 'life', 'got', \"'ll\", 'love', 'one', 'lives', 'know', \"'ve\", 'time', \"'re\"]\n",
      "\n",
      "\n",
      "cluster:  23\n",
      "theme:  ['always', \"'s\", \"n't\", \"'ll\", 'love', \"'m\", \"'re\", 'way', 'know', 'never', 'one', 'see', \"'ve\", 'go', 'like']\n",
      "\n",
      "\n",
      "cluster:  24\n",
      "theme:  ['world', \"'s\", \"n't\", \"'m\", \"'re\", 'know', 'love', 'got', 'one', \"'ve\", \"'ll\", 'see', 'like', 'na', 'get']\n",
      "\n",
      "\n",
      "cluster:  25\n",
      "theme:  ['woman', \"'s\", \"n't\", 'got', 'love', 'like', 'man', \"'m\", 'know', 'ai', \"'ll\", 'oh', 'na', 'time', 'gon']\n",
      "\n",
      "\n",
      "cluster:  26\n",
      "theme:  ['oh', \"n't\", \"'s\", 'love', \"'m\", \"'re\", 'know', 'like', \"'ll\", 'baby', 'yeah', 'let', 'never', \"'ve\", 'one']\n",
      "\n",
      "\n",
      "cluster:  27\n",
      "theme:  ['change', \"'s\", \"n't\", \"'ve\", 'know', \"'m\", 'time', 'mind', 'love', 'world', 'changing', \"'ll\", 'never', 'one', 'way']\n",
      "\n",
      "\n",
      "cluster:  28\n",
      "theme:  [\"n't\", \"'s\", \"'m\", 'get', 'like', 'got', 'man', \"'re\", 'know', 'na', 'hey', \"'ll\", 'little', 'well', 'go']\n",
      "\n",
      "\n",
      "cluster:  29\n",
      "theme:  ['la', 'love', \"'s\", \"n't\", 'oh', 'come', \"'re\", \"'m\", 'like', 'know', 'baby', 'time', 'get', 'see', 'life']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "#pp.pprint(themes)\n",
    "#print(themes)\n",
    "\n",
    "def print_themes(theme):\n",
    "    for idx, t in theme.items():\n",
    "        print( \"cluster: \", str(idx))\n",
    "        print( \"theme: \", t)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "print_themes(themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "More iteration on KMeans to discover a good K for this corpus is a good next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classification of new lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was over in Aberdeen\n",
      "On my way to New Orlean\n",
      "I was over in Aberdeen\n",
      "On my way to New Orlean\n",
      "Them Aberdeen women told me\n",
      "Will buy my gasoline\n",
      "\n",
      "Hey, two little women\n",
      "That I ain't ever seen\n",
      "They has two little women\n",
      "That I ain't never seen\n",
      "These two little women\n",
      "Just from New Orlean\n",
      "\n",
      "Ooh, sittin' down in Aberdeen\n",
      "With New Orlean on my mind\n",
      "I'm sittin' down in Aberdeen\n",
      "With New Orlean on my mind\n",
      "Well, I believe them Aberdeen women\n",
      "Gonna make me lose my mind, yeah\n",
      "\n",
      "Aber-deen is my home\n",
      "But the mens don't want me around\n",
      "Aberdeen is my home\n",
      "But the men don't want me around\n",
      "They know I will take these women\n",
      "An take them outta town\n",
      "\n",
      "Listen, you Aberdeen women\n",
      "You know I ain't got no dime\n",
      "Oh-oh listen you women\n",
      "You know'd I ain't got no dime\n",
      "They been had the po' boy\n",
      "All up and down\n"
     ]
    }
   ],
   "source": [
    "# build a classifier using KNN\n",
    "new_lyric = docs[0]\n",
    "print(new_lyric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vectorize new lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x29300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 41 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lyric_vector = vectorizer.transform([new_lyric])\n",
    "new_lyric_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build classifier from currenly vectorized lyrics and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=10)\n",
    "classifier.fit(lyrics_features, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_classifier.pkl',\n",
       " 'knn_classifier.pkl_01.npy',\n",
       " 'knn_classifier.pkl_02.npy',\n",
       " 'knn_classifier.pkl_03.npy',\n",
       " 'knn_classifier.pkl_04.npy',\n",
       " 'knn_classifier.pkl_05.npy']"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump\n",
    "joblib.dump(classifier, 'knn_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_lyric_label = classifier.predict(new_lyric_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lyric_label # label 2: \"love\", \"baby\", \"make\", \"like\", \"take\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now do recommendations\n",
    "We know which cluster this song is in so we can check out the other songs in that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similar_indx = (km.labels_==new_lyric_label).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get recommendations\n",
    "Get the closest songs in vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "recommendations = []\n",
    "for i in similar_indx:\n",
    "    dist = sp.linalg.norm((new_lyric_vector - lyrics_features[i]).toarray())\n",
    "    recommendations.append((dist, docs[i]))\n",
    "recommendations = sorted(recommendations)\n",
    "print(len(recommendations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            distance          recommended \n",
      "       1.23670444121There is a house in New Orleans\n",
      "They call the Rising Sun\n",
      "And it's been the ruin of many a poor boy\n",
      "And, God, I know I'm one\n",
      "\n",
      "Oh mother tell my sister\n",
      "Don't do what I have done\n",
      "Tell her to show me how down in New Orleans\n",
      "They call it the Rising Sun\n",
      "They call it the Rising Sun\n",
      "\n",
      "My daddy was a tailor\n",
      "He sews on them new bluejeans\n",
      "And my mama, she was a drunkard, lord\n",
      "Drinkin' down in New Orleans\n",
      "Drinkin' down in New Orleans\n",
      "\n",
      "Now the only thing a gambler needs\n",
      "Is a suitcase and trunk\n",
      "And the only time he's satisfied\n",
      "Is when he's on a drunk\n",
      "\n",
      "I'm going back to New Orleans\n",
      "My race is almost run\n",
      "I don't want to spend the rest of my live long days\n",
      "Beneath the Rising Sun\n",
      "Beneath the Rising Sun\n",
      "\n",
      "There is a house in New Orleans\n",
      "They call the Rising Sun\n",
      "And it's been the ruin of many a poor boy\n",
      "And, God, I know I'm one\n",
      "\n",
      "\n",
      "       1.25522589042There is a house in New Orleans\n",
      "They call the Rising Sun\n",
      "And it's been the ruin of many a poor boy\n",
      "And God, I know I'm one\n",
      "\n",
      "My mother was a tailor\n",
      "Sewed my new blue jeans\n",
      "My father was a gamblin' man\n",
      "Down in New Orleans\n",
      "\n",
      "Now the only thing a gambler needs\n",
      "Is a suitcase and a trunk\n",
      "And the only time that he's satisfied\n",
      "Is when he's on a drunk\n",
      "\n",
      "Oh mother, tell your children\n",
      "Not to do what I have done\n",
      "Spend your lives in sin and misery\n",
      "In the House of the Rising Sun\n",
      "\n",
      "Well, I've got one foot on the platform\n",
      "And the other foot on the train\n",
      "I'm going back to New Orleans\n",
      "To wear that ball and chain\n",
      "\n",
      "Well, there is a house in New Orleans\n",
      "They call the Rising Sun\n",
      "And it's been the ruin of many young poor boy\n",
      "And God, I know I'm one\n",
      "\n",
      "\n",
      "       1.25522589042There is a house in New Orleans\n",
      "They call the Rising Sun\n",
      "And it's been the ruin of many a poor boy\n",
      "And God, I know I'm one\n",
      "\n",
      "My mother was a tailor\n",
      "Sewed my new blue jeans\n",
      "My father was a gamblin' man\n",
      "Down in New Orleans\n",
      "\n",
      "Now the only thing a gambler needs\n",
      "Is a suitcase and trunk\n",
      "And the only time that he's satisfied\n",
      "Is when he's on a drunk\n",
      "\n",
      "Oh mother, tell your children\n",
      "Not to do what I have done\n",
      "Spend your lives in sin and misery\n",
      "In the House of the Rising Sun\n",
      "\n",
      "Well, I've got one foot on the platform\n",
      "The other foot on the train\n",
      "I'm going back to New Orleans\n",
      "To wear that ball and chain\n",
      "\n",
      "Well, there is a house in New Orleans\n",
      "They call the Rising Sun\n",
      "And it's been the ruin of many young poor boy\n",
      "And God, I know I'm one\n",
      "\n",
      "\n",
      "        1.3097797226Down around Biloxi\n",
      "Pretty girls are swimmin' in the sea\n",
      "Oh they all look like sisters in the ocean\n",
      "The boy will fill his pail with salted water\n",
      "And the storms will blow from off towards New Orleans\n",
      "\n",
      "The Sun shines on Biloxi\n",
      "The air is filled with vapors from the sea\n",
      "And the boy will dig a pool beside the ocean\n",
      "He sees creatures from a dream underwater\n",
      "And the sun will set from off towards New Orleans\n",
      "\n",
      "The stars can see Biloxi\n",
      "The stars can find their faces in the sea\n",
      "And we are walking in the evening by the ocean\n",
      "We are splashing naked in the water\n",
      "And the sky is red from off towards New Orleans\n",
      "\n",
      "\n",
      "       1.34799219662You filled my heatt with love\n",
      "It will never break apart\n",
      "Miracle in my eyes\n",
      "Time is running wild\n",
      "You know I have lost my way\n",
      "You know I have got to say\n",
      "Days like barrIcades have come to an end\n",
      "\n",
      "A new day has begun\n",
      "A new day shines like the sun\n",
      "\n",
      "You give me songs to sing\n",
      "You give me dreams to dream\n",
      "All I ever wanted I find here with you\n",
      "Forget the world is the world\n",
      "Forget the world is in pain\n",
      "Somehow it got out of hand\n",
      "Somehow I don't comprehend\n",
      "\n",
      "A new day has begun\n",
      "A new day shines like the sun\n",
      "\n",
      "Your lips are sweet\n",
      "It's all I need from life\n",
      "Love makes angels of us all\n",
      "I thought I would never meet a girl like you\n",
      "I know I will never let you go\n",
      "I will never let this go\n",
      "\n",
      "A new day has begun\n",
      "A new day shines like the sun\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_recommendations(records, num=5):\n",
    "    \"\"\" prints a table of recommended lyrics\"\"\"\n",
    "    for i in ['distance', 'recommended']:\n",
    "        print('{:>20s}'.format(i), end=' ')\n",
    "    print()\n",
    "    for cnt in range(num):\n",
    "        dist, song = records[cnt]\n",
    "        print('{:>20s}'.format(str(dist)), end='')\n",
    "        print('{:>20s}'.format(song), end='')\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    " \n",
    "\n",
    "print_recommendations(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The morning sun comes thru my window\n",
      "All night long I have been waiting\n",
      "We who are constantly moving\n",
      "Leaving part of us behind\n",
      "\n",
      "She moves across the room with easy grace\n",
      "Mona lisa smiles up on her face\n",
      "I who am completely mesmerized\n",
      "By the sunlight in her eyes\n",
      "\n",
      "Morning sun comes thru my windows\n",
      "All night long I have been waiting\n",
      "We who are constantly moving\n",
      "Leaving part of us behind\n",
      "\n",
      "Moves across the room with easy grace\n",
      "Mona lisa smiles up on her face\n",
      "I who am completely mesmerized\n",
      "By the sunlight in her eyes\n",
      "\n",
      "And the morning sun comes thru my window\n",
      "All night long I have been waiting\n",
      "We who are constantly moving\n",
      "Leaving part of us behind\n"
     ]
    }
   ],
   "source": [
    "print(recommendations[-1][1]) # last one in that cluster of 4080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. data cleaning and exploration is an iterative process. The problems I found in my data include: foreign languages, \n",
    "    non alpha-numberic character strings, repeated records, hard to remove stop words and more.\n",
    "2. Trying to make sense of the outcome and iterating on it. For instance, I rebuilt the clustering model a number of\n",
    "    times with different k values to come up with meaningful 'topics'\n",
    "3. Model evaluation is not straightforward for unsupervised learning. However, we can probably use a supervised approach \n",
    "   such as done above with knn and look at the outcome, or visualize the clusters as was done in topic modeling\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
